{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qn: Assume you are asked to get your code running in the cloud using AWS. What tools and AWS services would you use to deploy the API, database, and a scheduled version of your data ingestion code? Write up a description of your approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of AWS Tools and Services: \n",
    "Storage - Amazon Simple Storage Service (S3) \n",
    "Database - Amazon Relational Databse Service (RDS) \n",
    "Security & Access Management - AWS IAM\n",
    "Data Ingestion - AWS lambda, AWS Batch, Elastic Cloud Compute (EC2)\n",
    "Scheduling - AWS EventBridge \n",
    "API Setup & Hosting - Amazon Elastic Beasstalk and AWS Lambda\n",
    "Monitoring - AWS Cloudwatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Short Description of my approach to run this in AWS: \n",
    "\n",
    "Setup storage by creating a bucket in AWS S3\n",
    "Deploy RDS Instance for creating a database/table, store the data, and query or write various SQL statements\n",
    "Fetch endpoint information from RDS to setup API later\n",
    "Create IAM Roles and Policies, assign them to resources (batch jobs) and collaborators accordingly\n",
    "To setup Data Ingestion pipeline, create Lambda function to extract, transform and load,  deploy and then trigger the functions\n",
    "Setup REST API by deploying Flask and create an application in Elastic Beanstalk console. \n",
    "Convert Flask API to Lambda functions and use Amazon API Gateway to create/configure endpoints\n",
    "Use AWS Event Bridge to schedule the ingestion jobs \n",
    "Enable AWS Cloud Watch for monitoring the jobs\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
